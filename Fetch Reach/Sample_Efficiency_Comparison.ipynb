{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82c14042",
      "metadata": {
        "id": "82c14042"
      },
      "source": [
        "# Sample Efficiency Comparison of A2C, PPO, and SAC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3220441e",
      "metadata": {
        "id": "3220441e"
      },
      "source": [
        "\n",
        "This notebook compares the **sample efficiency** of three reinforcement learning algorithms — **A2C**, **PPO**, and **SAC** based on their performance in the **FetchReach-v3** environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298ad46b",
      "metadata": {
        "id": "298ad46b"
      },
      "source": [
        "## Evaluation Metrics\n",
        "The following indicators were used to assess performance:\n",
        "- **Mean reward** from evaluate_policy() (averaged over 100 episodes)\n",
        "- **Total reward** and **Euclidean distance** from a single manual trajectory\n",
        "- **Minimum distance** to goal achieved during that trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04e5cea",
      "metadata": {
        "id": "b04e5cea"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39868278",
      "metadata": {
        "id": "39868278"
      },
      "source": [
        "\n",
        "| Algorithm | Eval Mean Reward | Manual Mean Reward | Min Distance (Trajectory) | Total Reward | Total Distance |\n",
        "|-----------|------------------|---------------------|----------------------------|---------------|----------------|\n",
        "| **A2C** *(1M steps)* | -12.47 ± 2.70    | -12.59 ± 2.86       | 0.2821                     | -15.74        | 15.74          |\n",
        "| **SAC**   | -8.12 ± 3.44     | -7.63 ± 3.26        | 0.0527                    | -7.79         | 7.79           |\n",
        "| **PPO**   | -8.08 ± 2.53     | -8.24 ± 2.68        | 0.0827                    | -9.58         | 9.58           |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199c07ae",
      "metadata": {
        "id": "199c07ae"
      },
      "source": [
        "\n",
        "###  Note\n",
        "All algorithms were initially trained for **300,000 time steps**. However, **A2C was retrained with 1 million steps** due to its lower sample efficiency. Despite the extended training, it still underperformed relative to SAC and PPO in both reward and goal proximity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7888ae1a",
      "metadata": {
        "id": "7888ae1a"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "Based on the data:\n",
        "\n",
        "- **SAC** achieved the **lowest minimum distance to the goal** (0.0527), hitting the success threshold.\n",
        "- It also had the **best total reward** and **shortest accumulated distance** during the trajectory.\n",
        "- While PPO had similar evaluation performance to SAC, SAC reached the goal more precisely with better sample use when trained with the same number of timesteps.\n",
        "- **A2C**, even after more than triple the time steps in training, performed the worst overall, with a high minimum distance and lower rewards.\n",
        "\n",
        "###  Most Sample Efficient: **SAC**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}