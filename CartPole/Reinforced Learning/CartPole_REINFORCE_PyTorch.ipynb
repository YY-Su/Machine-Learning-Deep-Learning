{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBlpLiOYTl8O"
      },
      "source": [
        "# Implementing Policy Gradient with PyTorch on CartPole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu9sBwIUTl8W"
      },
      "source": [
        "## Monte-Carlo Policy Gradient (REINFORCE)\n",
        "\n",
        "Agent learning $\\pi_\\theta(a|s)$, where $\\theta$ is the parameter vector, $s$ is a particular state, and $a$ an action.\n",
        "\n",
        "In plain Monte-Carlo Policy Gradient, the agent finishes an entire episode first before updating the policy parameter based on the cumulative rewards obtained through the trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlzQOt0UTl8T"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3SGY4NDTl8V",
        "outputId": "91929ac6-7f39-485f-b498-5daf92632fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Observation space shape: (4,)\n",
            "Discrete(2)\n",
            "No of actions: 2\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Use CartPole Example\n",
        "#\n",
        "env = gym.make(\"CartPole-v1\", new_step_api=True, render_mode=\"human\")\n",
        "print(env.observation_space) #Box(Low, High, vector dim, in float32 values)\n",
        "print(\"Observation space shape:\",env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print(\"No of actions:\", env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2jTAqBTl8X"
      },
      "source": [
        "## Creating NN with PyTorch\n",
        "We create a simple NN:<br>\n",
        "Input - vector of 4 features:<br>\n",
        "\n",
        "**Cart Position (x)**:<br>\n",
        "The horizontal position of the cart on the track.\n",
        "Typically in the range of about ±4.8 (before termination).\n",
        "Positive values mean the cart is to the right, negative to the left.<br>\n",
        "**Cart Velocity (ẋ)**:<br>\n",
        "The linear velocity of the cart. Measured in m/s. Positive values indicate motion to the right, negative to the left.<br>\n",
        "**Pole Angle (θ):**<br>\n",
        "The angle of the pole from vertical (0 rad). Typically in the range ±0.209 rad (~12 deg) before the episode ends. Positive values mean the pole is leaning to the right, negative to the left.<br>\n",
        "**Pole Angular Velocity (θ̇):**<br>\n",
        "The rate of change of the pole's angle in rad/s. Positive values indicate counterclockwise rotation, negative means clockwise.<br>\n",
        "\n",
        "Neural Network:<br>\n",
        "Hidden Layer 1 - 20 nodes<br>\n",
        "Hidden Layer 2 - 30 nodes<br>\n",
        "Output Layer - 2 nodes (move left or right) <br>\n",
        "NN training to be done with Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICvS4H5rTl8X"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#Hyperparameters\n",
        "#\n",
        "LEARNING_RATE = 0.01\n",
        "GAMMA = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-LytjBUTl8Y"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# A model can be defined in PyTorch by subclassing the torch.nn.Module class.\n",
        "# This is the PyTorch base class meant to encapsulate behaviors specific to PyTorch Models and their components.\n",
        "#\n",
        "# The model is defined in two steps. We first specify the layer definition of the model,\n",
        "# and then outline how they are applied to the inputs. Here’s a simple model with\n",
        "# two linear layers and an activation function:\n",
        "#\n",
        "# class TinyModel(nn.Module):\n",
        "#    def __init__(self):\n",
        "#        super(TinyModel, self).__init__()\n",
        "#        self.linear1 = nn.Linear(D_in, H1) #Dim of input = D_in\n",
        "#        self.activation = nn.ReLU() # Dim of hidden layer = H1 (num of nodes)\n",
        "#        self.linear2 = nn.Linear(H1, D_out) #Dim of output = D_out\n",
        "#        self.softmax = nn.Softmax(dim=1)\n",
        "#\n",
        "#    def forward(self, x):\n",
        "#        x = self.linear1(x)\n",
        "#        x = self.activation(x)\n",
        "#        x = self.linear2(x)\n",
        "#        x = self.softmax(x)\n",
        "#        return x\n",
        "#\n",
        "# tinymodel = TinyModel()\n",
        "#\n",
        "# You may have noticed that we define the SoftMax activation for the final layer in this model.\n",
        "# This is because the CrossEntropyLoss function is not used here (remember we said that it has already\n",
        "# combined both a SoftMax activation and the cross entropy loss function inside).\n",
        "#\n",
        "# For PG, it is the sum of (log_prob*reward) of every steps. It is the objective function J(theta)\n",
        "# which we are attempting to maximize here.\n",
        "#\n",
        "class PolicyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(PolicyModel, self).__init__()\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(env.observation_space.shape[0], 20) # input dim=4, 20\n",
        "        self.linear2 = nn.Linear(20, 30) #hidden layer dimensions 20 & 30 (nodes) - 2 hidden layers\n",
        "        self.linear3 = nn.Linear(30, env.action_space.n) # output dim=2\n",
        "        self.activation = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1) #softmax over dim (col)\n",
        "\n",
        "        # Storages used during a trajectory\n",
        "        self.saved_log_probs = [] #stores ln(prob) of corresponding action chosen randomly during sampling in a trajectory\n",
        "        self.rewards = [] #stores rewards obtained during trajectory\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LvOr_t3Tl8Y"
      },
      "outputs": [],
      "source": [
        "model = PolicyModel() #instantiate the NN\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) #set the NN optimizer used in training,\n",
        "                                                             #the method parameters() comes from nn.Module class\n",
        "                                                             #i.e., we are training the model.parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBDHA86VTl8Z"
      },
      "source": [
        "### Select Action\n",
        "Chooses an action based on our policy probability distribution using the PyTorch distributions package. Returns a probability for each possible action in the action space (move left or move right) as an array (e.g. [0.7 0.3]). We then choose an action based on these probabilities, record our history, and return our action.<br>\n",
        "\n",
        "The PyTorch distribution package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization.<br>\n",
        "\n",
        "PyTorch supports REINFORCE by providing methods to create surrogate functions that can be backpropagated through. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning. When the probability density function is differentiable with respect to its parameters, we only need **sample()** and **log_prob()** to implement REINFORCE.<br>\n",
        "\n",
        "See [PyTorch documentation](https://pytorch.org/docs/stable/distributions.html):<br>\n",
        "\n",
        "\n",
        "```\n",
        "probs = policy_network(state) # e.g. output probs = [[0.3,0.7]]\n",
        "m = Categorical(probs)        # create a multinomial distribution based on probs\n",
        "action = m.sample()           # generate sample action corresponding to distribution\n",
        "next_state, reward = env.step(action)\n",
        "\n",
        "# m.log_prob(action) generate ln of prob linked to the generated action\n",
        "# e.g. move left -> 0.3, ln(0.3)=-1.204\n",
        "loss = -m.log_prob(action) * reward\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI9aCfohTl8Z"
      },
      "outputs": [],
      "source": [
        "# Function outputs an appropriate action based on prob distribution\n",
        "# At the same time, calc the log_prob and saved into model's saved_log_probs[]\n",
        "#\n",
        "# state is an input vector [position, velocity, angle, angular velocity] in numpy format.\n",
        "# Need to change this to torch tensor format.\n",
        "#\n",
        "def select_action(state):\n",
        "    # get output of prob of action from NN using input state s\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0) #convert to torch tensor, need to be array of arrays - unsqueeze(0)\n",
        "    probs = model.forward(state) #input is array of arrays, requirement of PyTorch, probs is e.g. [[0.3, 0.7]], call forward\n",
        "                                 #so, output is an array of array of probs.\n",
        "    # generate sample action corresponding to m distribution\n",
        "    m = Categorical(probs) # create a surrogate multinomial distribution based on probs\n",
        "    action = m.sample() #action (0 or 1) generated in torch tensor format, randomly according to m distribution\n",
        "\n",
        "    # calc log prob of corresponding sample action\n",
        "    model.saved_log_probs.append(m.log_prob(action)) #e.g. 0.3 chance of moving left, ln(0.3)=-1.204\n",
        "\n",
        "    return action.item() #return int value taken out from torch tensor, which represents the action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQVX9MdTl8a"
      },
      "source": [
        "###Reward $R_t$\n",
        "We update our policy by taking a sample of the action value function $Q{\\pi_\\theta} (s_t,a_t)$ by playing through episodes of the game.  $Q{\\pi_\\theta} (s_t,a_t)$ is defined as the expected return by taking action $a$ in state $s$ following policy $\\pi$.\n",
        "\n",
        "We know that for every step the simulation continues we receive a reward of 1.  We can use this to calculate the policy gradient at each time step, where $r$ is the reward for a particular state-action pair.  Rather than using the instantaneous reward, $r$, we instead use a long term reward $ R_{t} $ where $R_t$ is the discounted sum of all future rewards for the length of the episode.  In this way, the **longer** the episode runs into the future, the **greater** the reward for a particular state-action pair in the present. $R_{t}$ is then,\n",
        "\n",
        "$$ R_{t} = \\sum_{k=0}^{N} \\gamma^{k}r_{t+k} $$\n",
        "\n",
        "where $\\gamma$ is the discount factor (e.g., 0.99).  For example, an episode with 5 steps will have its rewards calculated as: [4.90, 3.94, 2.97, 1.99, 1]. This is assuming the agent is collecting a reward value of 1 at a\n",
        "each step.\n",
        "\n",
        "```\n",
        "r: (1)+0.99+0.99^2+0.99^3+0.99^4   (1)+0.99+0.99^2+0.99^3   (1)+0.99+0.99^2   (1)+0.99   (1)\n",
        "```\n",
        "\n",
        "Next we scale our reward vector by substracting the mean from each element and scaling to unit variance by dividing by the standard deviation. That is, we normalize the rewards. It also has the effect of compensating for future uncertainty as well as helping in NN convergence.\n",
        "\n",
        "## Update Policy\n",
        "After each episode we apply Monte-Carlo Policy Gradient to improve our policy according to the equation depicted in the Policy Gradient Theorem:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\, \\log \\pi_\\theta (a_t|s_t) . R_t  $$\n",
        "\n",
        "We will then feed our policy history multiplied by our rewards to our optimizer and update the weights of our neural network using stochastic gradent *ascent*.  This should increase the likelihood of actions that got our agent a larger reward. We also normalize our reward values.<br>\n",
        "\n",
        "This is a typical PyTorch NN parameter update training loop:\n",
        "```\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in data_loader:\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        outputs = net(inputs) # forward pass\n",
        "        loss = criterion(outputs, targets) #Compute loss\n",
        "        loss.backward() #Backward pass\n",
        "        optimizer.step() #Update parameters\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of how rewards are accumulated and calculated in a trajectory:\n",
        "\n",
        "```\n",
        "R = 0\n",
        "rewards = []\n",
        "for r in [1,1,1,1,1]: #rewards\n",
        "    R = r + 0.99*R\n",
        "    rewards.insert(0, R)\n",
        "```\n",
        "In the loop, rewards will change accordingly:\n",
        "```\n",
        "[1]\n",
        "[1.99, 1]\n",
        "[2.97, 1.99, 1]\n",
        "[3.94, 2.97, 1.99, 1]\n",
        "[4.90, 3.94, 2.97, 1.99, 1]\n",
        "\n",
        "r: (1)+0.99+0.99^2+0.99^3+0.99^4   (1)+0.99+0.99^2+0.99^3   (1)+0.99+0.99^2   (1)+0.99   (1)\n",
        "```"
      ],
      "metadata": {
        "id": "pVLDcFfMwxdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example to show torch.cat in action (tutorial refresh)\n",
        "test_list=[]\n",
        "for i in range(5): test_list.append(torch.tensor([i]))\n",
        "print(test_list) #list of torch tensor\n",
        "t=torch.cat(test_list) #t becomes a torch tensor tensor, concat all the numbers into 1 torch array\n",
        "print(t, type(t))\n",
        "print(t.sum()) # result is still a torch tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evZWHmm210J0",
        "outputId": "94c875cb-745f-4ccf-dc52-cee9eed4c69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0]), tensor([1]), tensor([2]), tensor([3]), tensor([4])]\n",
            "tensor([0, 1, 2, 3, 4]) <class 'torch.Tensor'>\n",
            "tensor(10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbZQRVJ2Tl8a"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Called per episode (trajectory)\n",
        "#\n",
        "def update_policy():\n",
        "    R = 0\n",
        "    policy_loss = [] # this is a bit of a misnomer\n",
        "    rewards = []\n",
        "    for r in model.rewards[::-1]: # model.rewards[] mostly contain 1s for CartPole, it's inside model object\n",
        "        R = r + GAMMA * R\n",
        "        rewards.insert(0, R) #contains all rewards (including discounted ones)\n",
        "\n",
        "    # Normalize reward\n",
        "    rewards = torch.tensor(rewards)\n",
        "    rewards = (rewards - rewards.mean())/(rewards.std() + 1e-10) # normalize for better convergence\n",
        "\n",
        "    # get loss\n",
        "    for reward, log_prob in zip(rewards, model.saved_log_probs):\n",
        "        #\n",
        "        # Note: Loss function is not cross-entropy like in regular NN\n",
        "        #       For PG, it is the sum of (log_prob*reward) of every steps that we are maximizing\n",
        "        #       Actually, it's a misnomer to call it a \"policy loss\"\n",
        "        #\n",
        "        policy_loss.append(-log_prob * reward) #to be concatenated and summed below cummulatively\n",
        "                                               #got to add a negative here cause Torch uses grad descent\n",
        "                                               #adding negative turns it into grad ascent\n",
        "                                               #policy_loss is a list of *torch tensors*\n",
        "                                               #Note: model.saved_log_probs is a list of\n",
        "                                               #      torch.tensor([x]) with []. It came from\n",
        "                                               #      m.log_prob(action)\n",
        "    #update NN parameters\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss_t = torch.cat(policy_loss).sum() #no need forward pass here as it has been called in select_action during trajectory\n",
        "                                                 #result torch.cat(policy_loss).sum() is a torch tensor\n",
        "    policy_loss_t.backward() #NN backpropagation\n",
        "    optimizer.step() #update NN parameters per episode iteration\n",
        "\n",
        "    # Empty out model.rewards[] & model.saved_log_probs[]\n",
        "    # to reaccumulate for next trajectory\n",
        "    del model.rewards[:] #delete everything elements in list leaving list empty\n",
        "    del model.saved_log_probs[:] #delete everything elements in list leaving list empty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9tj_SOTl8b"
      },
      "source": [
        "### Training\n",
        "This is our main policy training loop.  For each step in a training episode, we choose an action, take a step through the environment, and record the resulting new reward.  We call update_policy() at the end of each episode to feed the episode history to our neural network and improve our policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFVD1ArvTl8b"
      },
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3eTgAJ-OD0A",
        "outputId": "ae4c511f-a276-45e0-b15a-a42a0d0dc239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3zoP93CTl8b",
        "outputId": "5c9601f2-c1f4-4301-c897-04ede9ba4e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Rewards in this eps = 18.0, Avg rewards per eps=23.15\n",
            "Episode 40: Rewards in this eps = 35.0, Avg rewards per eps=24.57\n",
            "Episode 60: Rewards in this eps = 54.0, Avg rewards per eps=45.57\n",
            "Episode 80: Rewards in this eps = 22.0, Avg rewards per eps=76.06\n",
            "Episode 100: Rewards in this eps = 94.0, Avg rewards per eps=74.96\n",
            "Episode 120: Rewards in this eps = 64.0, Avg rewards per eps=72.74\n",
            "Episode 140: Rewards in this eps = 142.0, Avg rewards per eps=77.40\n",
            "Episode 160: Rewards in this eps = 160.0, Avg rewards per eps=84.23\n",
            "Episode 180: Rewards in this eps = 194.0, Avg rewards per eps=89.67\n",
            "Episode 189: Rewards in this eps = 500.0, Avg rewards per eps=100.65\n"
          ]
        }
      ],
      "source": [
        "from os import stat\n",
        "total_rewards=0\n",
        "episode=0\n",
        "\n",
        "while (True):\n",
        "    episode+=1\n",
        "    rewards_per_episode=0\n",
        "    s = env.reset()\n",
        "\n",
        "    # Run 1 episode\n",
        "    while (True):\n",
        "        # Inside select_action, we:\n",
        "        # (1) Model predict the softmax probs given a state\n",
        "        # (2) Generate a sample action example based on the softmax probs output\n",
        "        # (3) Take the opportunity to calc log prob of the corresponding sample action and\n",
        "        #     append it inside model.saved_log_probs[], e.g. 0.3 chance of moving left, ln(0.3)=-1.204\n",
        "        a = select_action(s)\n",
        "\n",
        "        # terminated - whether the episode has ended due to a terminal state (e.g., completion or failure)\n",
        "        # truncated - whether the episode has ended due to a time limit or other non-terminal conditions\n",
        "        #             (e.g., max no of steps has been reached). In CartPole, the episode might be truncated\n",
        "        #             if the pole stays upright for too long (e.g., 500 steps).\n",
        "        s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        model.rewards.append(r)\n",
        "        rewards_per_episode += r\n",
        "\n",
        "        if terminated or truncated: break\n",
        "\n",
        "    total_rewards += rewards_per_episode\n",
        "\n",
        "    if rewards_per_episode==500.0:\n",
        "        print(f\"Episode {episode}: Rewards in this eps = {rewards_per_episode}, Avg rewards per eps={total_rewards/episode:0.2f}\")\n",
        "        torch.save(model, '/content/drive/MyDrive/policyNet.pt')\n",
        "        break #ends when 500 is reached\n",
        "\n",
        "    if episode % 20 == 0:\n",
        "        print(f\"Episode {episode}: Rewards in this eps = {rewards_per_episode}, Avg rewards per eps={total_rewards/episode:0.2f}\")\n",
        "        torch.save(model, '/content/drive/MyDrive/policyNet.pt')\n",
        "\n",
        "    #\n",
        "    # UPDATE POLICY\n",
        "    #\n",
        "    update_policy()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}