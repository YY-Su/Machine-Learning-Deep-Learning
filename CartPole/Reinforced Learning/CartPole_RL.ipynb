{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GYM"
      ],
      "metadata": {
        "id": "F071iKM38Ap1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the help of the given CartPole_REINFORCE_PyTorch.ipynb code,\n",
        "train your RL model to achieve the max reward of 500.**\n",
        "\n",
        "**Verify that the model can achieve a reward of 500.\n",
        "You can do this by loading the trained model before running 100\n",
        "episodes with it to find the average reward per episode for these 100\n",
        "episodes. Print the\n",
        "rewards obtained at every 10 episodes.**"
      ],
      "metadata": {
        "id": "gnzqaJwqkvhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(gym.envs.registry.keys()) # To check all env available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bjl4p0r79GA",
        "outputId": "192d7757-aab2-4ee1-f4f9-30b2b15983af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHCMSaerNHyX"
      },
      "source": [
        "## Setup Gym Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v6AlmqUNHyY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Depending on the env:\n",
        "        - None (default): no render is computed.\n",
        "        - human: render return None.\n",
        "          The environment is continuously rendered in the current display or terminal. Usually for human consumption.\n",
        "        - rgb_array: return a single frame representing the current state of the environment.\n",
        "          A frame is a numpy.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n",
        "        - rgb_array_list: return a list of frames representing the states of the environment since the last reset.\n",
        "          Each frame is a numpy.ndarray with shape (x, y, 3), as with `rgb_array`.\n",
        "        - ansi: Return a strings (str) or StringIO.StringIO containing a\n",
        "          terminal-style text representation for each time step.\n",
        "          The text can include newlines and ANSI escape sequences (e.g. for colors).\n",
        "'''\n",
        "env = gym.make(\"CartPole-v1\", new_step_api=True, render_mode=\"human\") # modes avail: ['human', 'rgb_array', 'single_rgb_array']\n",
        "                                                                      # 'human' means video is off, only verbose output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# A model can be defined in PyTorch by subclassing the torch.nn.Module class.\n",
        "# This is the PyTorch base class meant to encapsulate behaviors specific to PyTorch Models and their components.\n",
        "#\n",
        "# The model is defined in two steps. We first specify the layer definition of the model,\n",
        "# and then outline how they are applied to the inputs. Hereâ€™s a simple model with\n",
        "# two linear layers and an activation function:\n",
        "#\n",
        "# class TinyModel(nn.Module):\n",
        "#    def __init__(self):\n",
        "#        super(TinyModel, self).__init__()\n",
        "#        self.linear1 = nn.Linear(D_in, H1) #Dim of input = D_in\n",
        "#        self.activation = nn.ReLU() # Dim of hidden layer = H1 (num of nodes)\n",
        "#        self.linear2 = nn.Linear(H1, D_out) #Dim of output = D_out\n",
        "#        self.softmax = nn.Softmax(dim=1)\n",
        "#\n",
        "#    def forward(self, x):\n",
        "#        x = self.linear1(x)\n",
        "#        x = self.activation(x)\n",
        "#        x = self.linear2(x)\n",
        "#        x = self.softmax(x)\n",
        "#        return x\n",
        "#\n",
        "# tinymodel = TinyModel()\n",
        "#\n",
        "# You may have noticed that we define the SoftMax activation for the final layer in this model.\n",
        "# This is because the CrossEntropyLoss function is not used here (remember we said that it has already\n",
        "# combined both a SoftMax activation and the cross entropy loss function inside).\n",
        "#\n",
        "# For PG, it is the sum of (log_prob*reward) of every steps. It is the objective function J(theta)\n",
        "# which we are attempting to maximize here.\n",
        "#\n",
        "class PolicyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(PolicyModel, self).__init__()\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(env.observation_space.shape[0], 20) # input dim=4, 20\n",
        "        self.linear2 = nn.Linear(20, 30) #hidden layer dimensions 20 & 30 (nodes) - 2 hidden layers\n",
        "        self.linear3 = nn.Linear(30, env.action_space.n) # output dim=2\n",
        "        self.activation = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1) #softmax over dim (col)\n",
        "\n",
        "        # Storages used during a trajectory\n",
        "        self.saved_log_probs = [] #stores ln(prob) of corresponding action chosen randomly during sampling in a trajectory\n",
        "        self.rewards = [] #stores rewards obtained during trajectory\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i5LdocwLvYZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained PyTorch model from disk\n",
        "model = torch.load(\"/content/drive/MyDrive/policyNet.pt\", weights_only=False)\n",
        "\n",
        "# Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5aWCev0zyue",
        "outputId": "03dfc84d-ad8f-493f-8528-be7b8282abad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyModel(\n",
              "  (linear1): Linear(in_features=4, out_features=20, bias=True)\n",
              "  (linear2): Linear(in_features=20, out_features=30, bias=True)\n",
              "  (linear3): Linear(in_features=30, out_features=2, bias=True)\n",
              "  (activation): ReLU()\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize total rewards across all episodes\n",
        "Total_Rewards = 0\n",
        "\n",
        "# Run the evaluation for 100 episodes\n",
        "for num_episode in range(1, 101):  # Run for 100 episodes\n",
        "    rewards = 0 # Track cumulative reward for the current episode\n",
        "    state = env.reset() # Reset environment to start a new episode\n",
        "    done = False # Flag to track end of episode\n",
        "\n",
        "    # Run the episode until it's done\n",
        "    while not done:\n",
        "        # Convert current state to a tensor and add batch dimension\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        # Predict action probabilities using the model (no gradient needed)\n",
        "        with torch.no_grad():\n",
        "            action_probs = model(state_tensor)\n",
        "\n",
        "        # Select the action with the highest probability\n",
        "        action = torch.argmax(action_probs, dim=1).item()\n",
        "\n",
        "        # Perform the selected action in the environment\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # Episode ends if either termination or truncation is true\n",
        "        done = terminated or truncated\n",
        "        rewards += reward # Accumulate the reward\n",
        "\n",
        "    # Print every 10th episode's reward\n",
        "    Total_Rewards += rewards\n",
        "    if num_episode % 10 == 0:\n",
        "        print(f\"Episode {num_episode}: {rewards}\")\n",
        "\n",
        "# Print average reward over all episodes\n",
        "print(\"Average\", Total_Rewards / 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbbt5SFp-KaI",
        "outputId": "085f344d-2c28-41d5-c19d-efc4ee7df71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: 500.0\n",
            "Episode 20: 500.0\n",
            "Episode 30: 500.0\n",
            "Episode 40: 500.0\n",
            "Episode 50: 500.0\n",
            "Episode 60: 500.0\n",
            "Episode 70: 500.0\n",
            "Episode 80: 500.0\n",
            "Episode 90: 472.0\n",
            "Episode 100: 500.0\n",
            "Average 494.54\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}