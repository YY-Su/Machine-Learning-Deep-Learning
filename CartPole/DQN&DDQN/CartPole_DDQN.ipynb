{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYmx6jJVO8r7"
      },
      "source": [
        "# Deep Q-Network(DDQN)\n",
        "We train NN in experience-replay mode<br>\n",
        "This is done at each step during learning <br>\n",
        "Training NN done in batches<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L-bM5p5AW1e",
        "outputId": "f21b6eab-cd92-4673-9ebf-eb6faaaebbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A_Z5cW6aO8sC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque # Double-Ended Queue which can add and remove elements from both ends. Supports both FIFO and LIFO.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5YLe_JpjwWq",
        "outputId": "67a497b3-9c6a-4d6c-90cd-0672020e8a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "(4,)\n",
            "4 State space\n",
            "2 Action space\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# State output is [Pos Velocity Angle Angular-Velocity] for Cartpole\n",
        "#\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.shape)\n",
        "print(env.observation_space.shape[0],'State space')\n",
        "print(env.action_space.n,'Action space')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfpQliIokVpC"
      },
      "source": [
        "##Create Dense Neural Network (NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz_s5GmbkM9o"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Note: input_dim=4 is same as input_shape=(4,)\n",
        "# Input is 4 nodes (from State space - vector of 4), 2 hidden layer of 32 nodes each, output 2 nodes\n",
        "#\n",
        "# So, input is s, when we predict it gives us highest Q-value with its associated action\n",
        "# env.observation_space.shape[0] is 4\n",
        "# env.action_space.n is 2\n",
        "#\n",
        "def model_init():\n",
        "  model=Sequential()\n",
        "  model.add(Dense(32, input_dim=env.observation_space.shape[0], activation='relu')) # input is 4\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(env.action_space.n, activation='linear')) # 2 output nodes, activation is linear for regression problem\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=1.0), loss='mse') # auto clips gradients tau before updating wts\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "online_Model = model_init() # Create the online model\n",
        "target_Model = model_init() # Create the target model\n",
        "target_Model.set_weights(online_Model.get_weights())  # Copy initial weights"
      ],
      "metadata": {
        "id": "GTtxssKoo5Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwj1iLjKlHSB"
      },
      "source": [
        "##Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoH-rfQtlQV4"
      },
      "outputs": [],
      "source": [
        "#-------------\n",
        "# Parameters\n",
        "#-------------\n",
        "\n",
        "GAMMA = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "MEMORYSIZE = 100000\n",
        "BATCHSIZE = 64\n",
        "C = 1000 # we update and sync target NN weights every C steps\n",
        "\n",
        "#Train till 200 reward, then run for 100?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otVyXZRBlqez"
      },
      "outputs": [],
      "source": [
        "memory = deque(maxlen=MEMORYSIZE) #using deque (FIFO) for experience-replay memory\n",
        "score = deque(maxlen=100) #keep track of scores of consecutive 100 episodes as defined in the winning requirement of Cartpole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW598GPloj7_"
      },
      "source": [
        "##Q-Learning for reference purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqkdU_9joevO"
      },
      "outputs": [],
      "source": [
        "#For each episode:\n",
        "#    init s (reset env)\n",
        "#\n",
        "#    For each step till done:\n",
        "#        execute eps-greedy policy to choose an action\n",
        "#\n",
        "#        sample next state s' and reward r based on chosen action\n",
        "#\n",
        "#        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s',:]) - Q[s,a]) #update Q table\n",
        "#\n",
        "#        s = s' # that is, we now move to state s'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to copy weights from primary network to target network\n",
        "def hard_update_target_model():\n",
        "    target_Model.set_weights(online_Model.get_weights())"
      ],
      "metadata": {
        "id": "N2wlv0mf6PNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLelzN03o8NO"
      },
      "source": [
        "##Function to implement experience-replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ5mkaiGo55M"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Replay in batch mode to train NN\n",
        "# (Train NN with 1 batch of random samples from replay buffer)\n",
        "#\n",
        "def experience_replay():\n",
        "    if len(memory) < BATCHSIZE: return #only start replay after memory has at least BATCHSIZE samples\n",
        "\n",
        "    xBatch = []\n",
        "    yBatch = []\n",
        "\n",
        "    #\n",
        "    # Preparing xBatch and yBatch so as to train the NN model\n",
        "    # That is, train NN to map states to QValues_sa\n",
        "    #\n",
        "    for s, a, r, s_, done in random.sample(memory, BATCHSIZE): # select 64 random tuples of past experience out from memory\n",
        "                                                               # random.sample() returns a list of stuff\n",
        "                                                               # note: s in numpy array form [[...]], cause model.predict()\n",
        "                                                               #       demands this\n",
        "        # Find Target\n",
        "        #\n",
        "        # target = r + gamma*np.max(Q[s',:])\n",
        "        # note: gamma*np.max(Q[s',:]) is the discounted future reward\n",
        "        #       also, this is simple DQN, no target network used, use back only primary network\n",
        "\n",
        "        # Modified for DDQN\n",
        "        if not done:\n",
        "            # Online Model selects best action\n",
        "            best_action = np.argmax(online_Model.predict(s_, verbose=0)[0])  # Select action using Online Model\n",
        "            # Target Model evaluates the best action's Q-value\n",
        "            target = r + GAMMA * target_Model.predict(s_, verbose=0)[0][best_action] # Use Target Model to evaluate\n",
        "        else:\n",
        "            target = r\n",
        "\n",
        "        # Assign target calculated to the correct Q[s,a] (there are 2 actions, one of them gives you max value)\n",
        "        # model.predict(s)[0] gives you output vector of 2 Q-values for the 2 possible actions\n",
        "        QValues_sa = online_Model.predict(s, verbose=0)[0] #Predict always output [[...]], we change this to [...] and assign to QValues_sa\n",
        "        QValues_sa[a] = target           #update it with new target value, QValues_sa is np array of shape (2,)\n",
        "\n",
        "        # Create x,y batches of \"ground truths\" to train NN\n",
        "        #\n",
        "        # each sample in xBatch is one state s, ie, input is a vector of 4\n",
        "        # with corresponding reward target (calculated ground truth)\n",
        "        xBatch.append(s[0]) #s already in numpy format, change from [[...]] to [...] -> contains a batch of states\n",
        "        yBatch.append(QValues_sa) #each QValues_sa is a vector of 2 output\n",
        "\n",
        "    # Train NN with 1 batch of 64 samples for 1 grad descent (backprop)\n",
        "    online_Model.fit(np.array(xBatch), np.array(yBatch), epochs=1, batch_size=BATCHSIZE, verbose=0) #do the NN training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVtA2HgkO8sf"
      },
      "source": [
        "## Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXbfJlVm3v6_"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# SHORT CUT: OVERRIDE AND LOAD MODEL WITH PRE-TRAINED PARAMETERS\n",
        "#            HELP SPEED UP TRAINING\n",
        "#\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load trained model\n",
        "online_Model = tf.keras.models.load_model('/content/drive/MyDrive/CartPole.keras')\n",
        "\n",
        "# Load last saved epsilon value\n",
        "#EPSILON = np.load('/content/drive/MyDrive/epsilon.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGzK0k3HO8sg",
        "outputId": "2716dedc-3451-428d-9585-771769ced85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Score: 76.0, Avg over last 100 tries: 76.0, eps=0.0100\n",
            "Episode: 2, Score: 12.0, Avg over last 100 tries: 44.0, eps=0.0100\n",
            "Episode: 3, Score: 10.0, Avg over last 100 tries: 32.7, eps=0.0100\n",
            "Episode: 4, Score: 17.0, Avg over last 100 tries: 28.8, eps=0.0100\n",
            "Episode: 5, Score: 9.0, Avg over last 100 tries: 24.8, eps=0.0100\n",
            "Episode: 6, Score: 9.0, Avg over last 100 tries: 22.2, eps=0.0100\n",
            "Episode: 7, Score: 9.0, Avg over last 100 tries: 20.3, eps=0.0100\n",
            "Episode: 8, Score: 9.0, Avg over last 100 tries: 18.9, eps=0.0100\n",
            "Episode: 9, Score: 9.0, Avg over last 100 tries: 17.8, eps=0.0100\n",
            "Episode: 10, Score: 10.0, Avg over last 100 tries: 17.0, eps=0.0100\n",
            "Episode: 11, Score: 9.0, Avg over last 100 tries: 16.3, eps=0.0100\n",
            "Episode: 12, Score: 10.0, Avg over last 100 tries: 15.8, eps=0.0100\n",
            "Episode: 13, Score: 10.0, Avg over last 100 tries: 15.3, eps=0.0100\n",
            "Episode: 14, Score: 13.0, Avg over last 100 tries: 15.1, eps=0.0100\n",
            "Episode: 15, Score: 92.0, Avg over last 100 tries: 20.3, eps=0.0100\n",
            "Episode: 16, Score: 95.0, Avg over last 100 tries: 24.9, eps=0.0100\n",
            "Episode: 17, Score: 155.0, Avg over last 100 tries: 32.6, eps=0.0100\n",
            "Episode: 18, Score: 227.0, Avg over last 100 tries: 43.4, eps=0.0100\n",
            "Training Complete! Episode 18 reached 200 reward.\n"
          ]
        }
      ],
      "source": [
        "# ====\n",
        "# MAIN\n",
        "# ====\n",
        "\n",
        "eps = 0.01\n",
        "i=0 # Episode i\n",
        "step_count = 0\n",
        "\n",
        "while True: # we continue each episode i till the winning condition is achieved\n",
        "    i=i+1\n",
        "    state = np.array([env.reset()]) # [[Pos Velocity Angle Angular-Velocity]], we use numpy array\n",
        "                                    # we make is an array of array because of model.predict() requires it\n",
        "    reward_per_episode = 0\n",
        "\n",
        "    ##\n",
        "    ## For each step inside an Episode\n",
        "    ##\n",
        "    for t in range(500): # each episode only has max 500 time steps in CartPole\n",
        "        #\n",
        "        # eps-greedy policy\n",
        "        #\n",
        "        step_count += 1 # Increment Step count\n",
        "\n",
        "        if np.random.rand() <= eps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(online_Model.predict(state, verbose=0)) #get indices with max value, do forward pass to predict\n",
        "                                                                #the max QValues_sa of the input state, and select\n",
        "                                                                #the associated action\n",
        "        # step to next state\n",
        "        state_, reward, done, _, _ = env.step(action) #  [...]\n",
        "        next_state = np.array([state_])               # [[...]], because of model.predict() at experience_replay()\n",
        "        reward_per_episode += reward\n",
        "\n",
        "        # Add to deque memory for experience replay - To train NN later\n",
        "        memory.append((state,action,reward,next_state,done)) # add into memory for experience replay as tuples\n",
        "\n",
        "\t      #\n",
        "\t      # Experience Replay at each time step to train the NN\n",
        "        # Use 1 batch of experiences from memory to train\n",
        "\t      # NN so that it will learn to map from a state to QValues_sa (output vector of 2)\n",
        "\t      #\n",
        "        experience_replay()\n",
        "\n",
        "        # **Sync target network every C steps**\n",
        "        if step_count % C == 0:\n",
        "            target_Model.set_weights(online_Model.get_weights())  # Update target network\n",
        "\n",
        "        if done:\n",
        "            hard_update_target_model()  # Sync Target Network\n",
        "            score.append(reward_per_episode)\n",
        "            print(f\"Episode: {i}, Score: {reward_per_episode}, Avg over last 100 tries: {sum(score)/len(score):.1f}, eps={eps:0.4f}\")\n",
        "            online_Model.save('/content/drive/MyDrive/CartPole.keras')  # Save model at end of each episode\n",
        "            np.save('/content/drive/MyDrive/epsilon.npy', eps)  # Save epsilon value\n",
        "\n",
        "            break\n",
        "\n",
        "        state = next_state #S = S'\n",
        "\n",
        "    # eps-decay\n",
        "    #if EPSILON > EPSILON_MIN: EPSILON *= EPSILON_DECAY\n",
        "\n",
        "    # STOP when exactly 200 reward is reached\n",
        "    if reward_per_episode >= 200:\n",
        "        print(f\"Training Complete! Episode {i} reached 200 reward.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then, with the trained model, run a 100 episodes (i.e., no more\n",
        "training needed here) to find the average reward per episode for these\n",
        "100 episodes. Print\n",
        "the rewards obtained at every 10 episodes.**"
      ],
      "metadata": {
        "id": "A2AIL8IP1OTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate trained model for 100 episodes\n",
        "rewards = []\n",
        "for ep in range(1, 101):\n",
        "    state = np.array([env.reset()])\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(online_Model.predict(state, verbose=0))\n",
        "        state_, reward, done, _, _ = env.step(action)\n",
        "        state = np.array([state_])\n",
        "        total_reward += reward\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "    # Print average reward every 10 episodes\n",
        "    if ep % 10 == 0:\n",
        "        avg = np.mean(rewards[ep-10:ep])\n",
        "        print(f\"Episode {ep-9} to {ep}: Avg Reward = {avg:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvLQAprgjk73",
        "outputId": "b520ea12-2463-4c49-f95a-f0329fefb424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 to 10: Avg Reward = 178.20\n",
            "Episode 11 to 20: Avg Reward = 175.50\n",
            "Episode 21 to 30: Avg Reward = 126.80\n",
            "Episode 31 to 40: Avg Reward = 221.80\n",
            "Episode 41 to 50: Avg Reward = 133.00\n",
            "Episode 51 to 60: Avg Reward = 162.80\n",
            "Episode 61 to 70: Avg Reward = 169.80\n",
            "Episode 71 to 80: Avg Reward = 174.90\n",
            "Episode 81 to 90: Avg Reward = 175.50\n",
            "Episode 91 to 100: Avg Reward = 169.20\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}