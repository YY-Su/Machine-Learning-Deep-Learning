{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfX6OPonBuU1"
      },
      "source": [
        "#Monte Carlo First Visit Method to solve a simple GridWorld problem.#\n",
        "\n",
        "Analyzing State-Value Convergence with First-Visit Monte Carlo\n",
        "<br>In this case, we are using a RANDOM UNIFORM POLICY to train.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhusOz-jGdIK",
        "outputId": "c14e3ee9-2c41-42bb-8d53-7473fea30d16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4xQ1BjKG4w5"
      },
      "source": [
        "import sys\n",
        "\n",
        "# where gridworld.py is located\n",
        "# modify path accordingly\n",
        "sys.path.append('/content/drive/MyDrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpDv_FjeCitt"
      },
      "source": [
        "import numpy as np\n",
        "import gridworld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2tBWQ-5Fjan"
      },
      "source": [
        "**Modify the code to enable the MC simulations to run 100, 500, 1000, 5000, and 10000 episodes. Run gridworld.print_values(state_values,env) to show the\n",
        "results for each case.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKTvEWGC36B"
      },
      "source": [
        "**Standard Grid:**<br>\n",
        "x means you can't go there<br>\n",
        "s means start position<br>\n",
        "number means reward at that state<br>\n",
        "**.  .  .  1**<br>\n",
        "**.  x  . -1**<br>\n",
        "**s  .  .  .**<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b3TC5VODR-6"
      },
      "source": [
        "# define a grid that describes the reward for arriving at each state and possible actions at each state\n",
        "# the grid looks like this:\n",
        "#\n",
        "# x means you can't go there\n",
        "# s means start position\n",
        "# number means reward at that state\n",
        "#   0  1  2  3\n",
        "# 0 .  .  .  1\n",
        "# 1 .  x  . -1\n",
        "# 2 s  .  .  .\n",
        "#\n",
        "# We go by (row, col)\n",
        "#\n",
        "EPISODE_COUNT = [100,500,1000,5000,10000]\n",
        "GAMMA = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mc_simulation(num_episodes):\n",
        "    \"\"\"\n",
        "    Runs a Monte Carlo (MC) simulation to estimate state values in the GridWorld environment.\n",
        "    \"\"\"\n",
        "    env = gridworld.standard_grid() # Initialize the 3x4 grid environment\n",
        "    returns = {}  # Dictionary to store returns for each state, where returns[s] is a list\n",
        "    state_values = {}  # Dictionary to store the estimated value function for each state\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()  # Reset environment to the start state\n",
        "        trajectory = []  # List to store the state-reward trajectory of one episode\n",
        "\n",
        "        while True:\n",
        "            action = np.random.choice(['U', 'D', 'L', 'R'])  # Choose a random action\n",
        "            next_state, reward, done, invalid = env.step(action)  # Take action and observe outcome\n",
        "            if invalid:\n",
        "                continue  # Skip invalid moves and try again\n",
        "\n",
        "            trajectory.append((state, reward))  # Store the state and received reward: St, Rt+1\n",
        "            state = next_state  # Move to the next state\n",
        "            if done:\n",
        "                break  # End episode if a terminal state is reached\n",
        "\n",
        "        reverse_trajectory = trajectory[::-1]  # Reverse the trajectory to process it backward\n",
        "        reverse_trajectory_states = list(map(lambda x: x[0], reverse_trajectory))  # Extract states\n",
        "        G = 0  # Initialize the return (discounted cumulative reward)\n",
        "\n",
        "        for idx, step in enumerate(reverse_trajectory):\n",
        "            s, r = step  # Extract state and reward\n",
        "            G = GAMMA * G + r  # Compute return using the discount factor\n",
        "             # say, after 3 steps, G=GAMMA*(GAMMA*(GAMMA*G + r3) + r2) + r1\n",
        "             #                     G=GAMMA^3*G + GAMMA^2*r3 + GAMMA*r2 + r1\n",
        "             #                     G=GAMMA^2*r3 + GAMMA*r2 + r1\n",
        "\n",
        "            # First-visit MC: Only update the first occurrence of the state in the episode\n",
        "            if s not in reverse_trajectory_states[idx+1:]:\n",
        "                if s in returns:\n",
        "                    returns[s].append(G)  # Append return to the list of returns for this state\n",
        "                else:\n",
        "                    returns[s] = [G]  # Create a new list for this state\n",
        "\n",
        "                state_values[s] = np.mean(returns[s])  # Compute the average return for this state\n",
        "\n",
        "        # Manually assign values for terminal states as they receive no further rewards\n",
        "        state_values[(0,3)] = 1  # Goal state with positive reward\n",
        "        state_values[(1,3)] = -1  # Penalty state with negative reward\n",
        "\n",
        "    return state_values  # Return the estimated state values"
      ],
      "metadata": {
        "id": "2GNmimyafuvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the episodes 100,500,1000,5000,10000 and print out the results.\n",
        "for episodes in EPISODE_COUNT:\n",
        "    print(f\"Running simulation for {episodes} episodes...\")\n",
        "    state_values = run_mc_simulation(episodes)\n",
        "    gridworld.print_values(state_values, gridworld.standard_grid())\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lJFvCIjf9XY",
        "outputId": "88c69e51-9828-43ab-896c-902baaf2cb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for 100 episodes...\n",
            "---------------------------\n",
            "-0.01| 0.09| 0.27| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.45|-1.00|\n",
            "---------------------------\n",
            "-0.15|-0.27|-0.44|-0.69|\n",
            "\n",
            "\n",
            "Running simulation for 500 episodes...\n",
            "---------------------------\n",
            " 0.07| 0.15| 0.30| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-0.32|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.21|-0.36|-0.65|\n",
            "\n",
            "\n",
            "Running simulation for 1000 episodes...\n",
            "---------------------------\n",
            " 0.04| 0.11| 0.24| 1.00|\n",
            "---------------------------\n",
            "-0.04| 0.00|-0.40|-1.00|\n",
            "---------------------------\n",
            "-0.12|-0.24|-0.40|-0.66|\n",
            "\n",
            "\n",
            "Running simulation for 5000 episodes...\n",
            "---------------------------\n",
            " 0.04| 0.14| 0.26| 1.00|\n",
            "---------------------------\n",
            "-0.03| 0.00|-0.36|-1.00|\n",
            "---------------------------\n",
            "-0.11|-0.21|-0.36|-0.68|\n",
            "\n",
            "\n",
            "Running simulation for 10000 episodes...\n",
            "---------------------------\n",
            " 0.05| 0.14| 0.26| 1.00|\n",
            "---------------------------\n",
            "-0.03| 0.00|-0.37|-1.00|\n",
            "---------------------------\n",
            "-0.11|-0.22|-0.38|-0.68|\n",
            "\n",
            "\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions from Running the Monte Carlo Simulation for 5 Cases\n",
        "\n",
        "1. **Early episodes (100 & 500) show high variance in values**\n",
        "\n",
        "  * The state values fluctuate significantly in the first two cases (100 and 500 episodes), because the agent has not explored the environment enough.\n",
        "  * Notable inconsistencies: (1,2) = -0.45 (100 episodes) â†’ -0.32 (500 episodes), indicate irregularities due to insufficient sampling.\n",
        "  * Estimates are unstable, and values change drastically between runs.\n",
        "\n",
        "2. **Convergence Begins (1000 Episodes)**\n",
        "\n",
        "  * Variance decreases, and values start stabilizing.\n",
        "  * Example: (1,2) settles around -0.40, (2,2) around -0.40.\n",
        "\n",
        "3. **Stabilization (5000+ Episodes)**\n",
        "  * Values become more consistent with minimal changes.\n",
        "    * Example: (1,2) = -0.36 (5000 episodes) â†’ -0.37 (10,000 episodes).\n",
        "  * Comparing 5000 and 10,000 episodes, we see only minor changes in the estimated values. This suggests that after around 5000 episodes, the state values have mostly converged.\n",
        "\n",
        "4. **Final Observation**\n",
        "  * Positive values appear closer to the goal state (1 at (0,3)), indicating higher expected rewards.\n",
        "  * Negative values appear near the penalty state (-1 at (1,3)), meaning these states lead to losses.\n",
        "  * The values suggest the agent learns to move towards the goal while avoiding the penalty.\n",
        "  * Final state values align with expected behavior\n",
        "    * Higher values near the goal (0.14, 0.26).\n",
        "    * Lower values near the penalty (-0.37, -0.38).\n"
      ],
      "metadata": {
        "id": "xRJD9bxzi-6y"
      }
    }
  ]
}