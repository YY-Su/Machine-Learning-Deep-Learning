{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfX6OPonBuU1"
      },
      "source": [
        "#Monte Carlo First Visit Method to solve a simple GridWorld problem.#\n",
        "\n",
        "Deriving a Greedy Policy from Monte Carlo State-Value Estimation.\n",
        "<br>In this case, we are using a RANDOM UNIFORM POLICY to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhusOz-jGdIK",
        "outputId": "1275d1d6-0826-4a74-e05d-411b270c7519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4xQ1BjKG4w5"
      },
      "source": [
        "import sys\n",
        "\n",
        "# where gridworld.py is located\n",
        "# modify path accordingly\n",
        "sys.path.append('/content/drive/MyDrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpDv_FjeCitt"
      },
      "source": [
        "import numpy as np\n",
        "import gridworld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2tBWQ-5Fjan"
      },
      "source": [
        "**Derive the policy to navigate the agent AFTER the agent has been trained through 10000 episodes. In this case, we will choose GREEDY actions that enable the agent to move to the states with the highest value.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKTvEWGC36B"
      },
      "source": [
        "**Standard Grid:**<br>\n",
        "x means you can't go there<br>\n",
        "s means start position<br>\n",
        "number means reward at that state<br>\n",
        "**.  .  .  1**<br>\n",
        "**.  x  . -1**<br>\n",
        "**s  .  .  .**<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b3TC5VODR-6"
      },
      "source": [
        "# define a grid that describes the reward for arriving at each state and possible actions at each state\n",
        "# the grid looks like this:\n",
        "#\n",
        "# x means you can't go there\n",
        "# s means start position\n",
        "# number means reward at that state\n",
        "#   0  1  2  3\n",
        "# 0 .  .  .  1\n",
        "# 1 .  x  . -1\n",
        "# 2 s  .  .  .\n",
        "#\n",
        "# We go by (row, col)\n",
        "#\n",
        "MAX_EPISODES = 10000\n",
        "GAMMA = 0.9\n",
        "ACTIONS = ['U', 'D', 'L', 'R']\n",
        "\n",
        "env = gridworld.standard_grid() #3x4 grid\n",
        "returns = {}  # Dictionary to store returns for each state, returns[s] points to a list\n",
        "state_values = {} # Dictionary to store state value functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Generate Episodes\n",
        "#\n",
        "for ep in range(MAX_EPISODES):\n",
        "\tstate = env.reset() #return a tuple, state at start\n",
        "\ttrajectory = [] #to store trajectory of 1 episode\n",
        "\n",
        "  #\n",
        "\t# generate 1 episode\n",
        "  #\n",
        "\twhile True:\n",
        "\t\t# using UNIFORM RANDOM as a policy to train (behavioral policy)\n",
        "\t\taction = np.random.choice(ACTIONS)\n",
        "\n",
        "\t\tnext_state, reward, done, invalid = env.step(action)\n",
        "\t\tif invalid: continue #go back and get another random action\n",
        "\n",
        "\t\ttrajectory.append((state, reward)) #St, Rt+1\n",
        "\t\tstate = next_state\n",
        "\t\tif done: break\n",
        "\n",
        "  #\n",
        "\t# Analyse the 1 episode generated\n",
        "\t# E.g. trajectory = [((2, 0), 0), ((2, 1), 0), ((2, 0), 0), ((2, 1), 0), ((2, 2), 0), ((1, 2), -1)]\n",
        "\t#\n",
        "\treverse_trajectory = trajectory[::-1]\n",
        "\treverse_trajectory_states = list(map(lambda x:x[0], reverse_trajectory)) # just the states only\n",
        "\tG = 0\n",
        "\n",
        "\t#\n",
        "\t# GO THRU trajectory BACKWARD to CALC averages\n",
        "\t# trajectory[] contains (state, reward) of only 1 episode\n",
        "\t#\n",
        "\tfor idx, step in enumerate(reverse_trajectory):\n",
        "\t\ts, r = step\n",
        "\t\tG = GAMMA*G + r # say, after 3 steps, G=GAMMA*(GAMMA*(GAMMA*G + r3) + r2) + r1\n",
        "                    #                     G=GAMMA^3*G + GAMMA^2*r3 + GAMMA*r2 + r1\n",
        "                    #                     G=GAMMA^2*r3 + GAMMA*r2 + r1\n",
        "\n",
        "    #\n",
        "    # First-visit MC: only update the FIRST OCCURENCE of the state\n",
        "    #                         5       4       3       2       1       0\n",
        "\t\t# E.g. for trajectory = [(2, 0), (2, 1), (2, 0), (2, 1), (2, 2), (1, 2)]\n",
        "\t\t#      it will be idx: 0, 1, 4, 5\n",
        "\t\tif s not in reverse_trajectory_states[idx+1:]:\n",
        "\t\t\t# we add state s to dict returns, returns[s] points to a list\n",
        "\t\t\tif s in returns: returns[s].append(G) # returns is a dict that points to lists\n",
        "\t\t\telse:            returns[s] = [G] # first guy in the list\n",
        "\n",
        "\t\t\t# update state_values[s] dict, which stores mean gain results of each state s\n",
        "\t\t\tstate_values[s] = np.mean(returns[s]) # returns[s] points to a list\n",
        "\n",
        "    # The terminal states will have 0 value cause no further rewards are received after reaching them.\n",
        "    # That is, agent doesn't move from terminal states to get further rewards. That is why we\n",
        "    # manually insert the 2 final values (rewards) in the terminal states\n",
        "\t\tstate_values[(0,3)]=1\n",
        "\t\tstate_values[(1,3)]=-1\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHdcVi6JPkv_",
        "outputId": "5acf1c3d-5562-4e14-8917-622f2d8b7566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to derive policy\n",
        "def choose_action(state):\n",
        "    \"\"\"\n",
        "    Given a state, choose the best action based on learned state values.\n",
        "    Uses a greedy policy to select the action that leads to the state with the highest value.\n",
        "\n",
        "    Parameters:\n",
        "    state (tuple): The current state (row, col) in the grid.\n",
        "\n",
        "    Returns:\n",
        "    str: The best action ('U', 'D', 'L', 'R') or 'X' for terminal states.\n",
        "    \"\"\"\n",
        "\n",
        "    # If the state is a terminal state, return 'X' (no action needed)\n",
        "    if state in env.terminal_states:\n",
        "        return 'X'\n",
        "\n",
        "    # Check if the state has available actions\n",
        "    if state in env.actions:\n",
        "        possible_actions = env.actions[state]  # Get possible actions\n",
        "        best_action = None  # Track the best action\n",
        "        best_value = 0.0  # Initialize the best value to 0\n",
        "\n",
        "        # Iterate over all possible actions to find the best one\n",
        "        for action in possible_actions:\n",
        "            env.reset()  # Reset environment to the given state\n",
        "            env.i, env.j = state  # Set the agent to the current state\n",
        "            next_state, _, _, invalid = env.step(action)  # Take action and observe next state, ignoring reward and done status.\n",
        "\n",
        "            if invalid:\n",
        "                continue  # Skip invalid moves\n",
        "\n",
        "            value = state_values.get(next_state, 0.0)  # Get state value, default to 0\n",
        "            if value > best_value:  # If this state has a higher value, update best action\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        # Return the best action, or a random valid action if all values are equal\n",
        "        return best_action if best_action else possible_actions[0]  # Always choose first action\n",
        "\n",
        "    return ' '  # Return empty space if no valid actions exist"
      ],
      "metadata": {
        "id": "3dzluEKm4DqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"State Values:\")\n",
        "gridworld.print_values(state_values, env)\n",
        "\n",
        "# Derive policy\n",
        "greedy_policy = {}\n",
        "for state in env.all_states():\n",
        "    greedy_policy[state] = choose_action(state)\n",
        "\n",
        "print(\"\\nDerived Optimal Policy:\")\n",
        "gridworld.print_policy(greedy_policy, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFf9NMQr4Hvt",
        "outputId": "a96289eb-2241-42e2-9a05-9d7b13632428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Values:\n",
            "---------------------------\n",
            " 0.05| 0.14| 0.27| 1.00|\n",
            "---------------------------\n",
            "-0.02| 0.00|-0.35|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.21|-0.37|-0.66|\n",
            "\n",
            "Derived Optimal Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  X  |\n",
            "---------------------------\n",
            "  U  |     |  U  |  X  |\n",
            "---------------------------\n",
            "  U  |  L  |  L  |  L  |\n"
          ]
        }
      ]
    }
  ]
}